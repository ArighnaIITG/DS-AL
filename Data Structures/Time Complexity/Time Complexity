Time Complexity of a program is the time taken for a program to run, given the input size keeps on increasing.

You can understand the need for time complexity analysis of a program by seeing the first video of TIME COMPLEXITY on "https://youtube.com/mycodeschool"

Running Time of an algorithm depends uopn :

	1. Single or multi-processor
	2. 32-bit or 64-bit processor
	3. Read/Write speed to memory
	4. Input to the computer

When we talk about time complexity, we take into account only the last option.
We are interested in calculating the rate of growth of time taken by an algorithm with respect to the input.

We first define a model machine acc. to our choice. Suppose the model machine takes 1 unit of time for basic mathematical operations, and 1 unit of time for assignment and return.

Let's take examples :

    1. Sum of two numbers :

       Pseudocode : Sum(a, b)
    			    { return a+b }

    	Here addition takes 1 unit of time and return also. Therefore, total = 2 units of time.
    	So, the rate will be same.
    	Therefore, it's a "constant function."

    2. Sum of all elements in a list :

    	Pseudocode : Sum(A, n)                                           COST      NO. OF TIMES
    				 {
    				 	total = 0                                         c1            1
    				 	for(int i=0; i<=n-1; i++)                         c2           n + 1
    				 			total += A[n]                             c3            n
    				 	return total                                      c4            1
    				 }

        So, total time: c1 + c2*(n+1) + c3*n + c4
        So, rate of growth is "linear" w.r.t to the input.

    3. Similarly, Sum of elements in a 2-dimensional matrix will be of the form 
       T(n) = an**2 + bn*2 + c.

    This can be replaced by Big-Oh notation , where quadratic notation can be represented by O(n**2)
    and linear can be represented by O(n). These are called as asymptotic bounds, or notations.
    More details in next slide.
